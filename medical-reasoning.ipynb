{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f20b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8e2173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?',\n",
       " 'Complex_CoT': \"Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\",\n",
       " 'Response': 'The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940e68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e8dd4f017f4164816821776931e621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "config._attn_implementation = \"eager\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\", config=config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\", use_fast=True)\n",
    "\n",
    "target_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6b9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"<think>{example['Complex_CoT']}</think>{example['Response']}\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo,tokenize=False,add_generation_prompt=False) for convo in convos]\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess_fn,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956e9e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "A 25-year-old woman presents to the ED with a diffuse, erythematous rash, nausea, vomiting, and fever for 2 days. Physical examination reveals a soaked tampon in her vagina, and blood cultures are negative, suggesting toxic shock syndrome. Which specific molecule on T cells does the toxin most likely bind to?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Alright, here's a situation with a 25-year-old woman who showed up in the emergency department. She's got this widespread red rash, feeling nauseous, she's vomiting, and running a fever for two days. Something's not quite right here, and it all starts connecting to the idea of toxic shock syndrome. Oh, and there's a crucial detail: they found a soaked tampon during her exam.\n",
      "\n",
      "Okay, let's dig into what's happening in toxic shock syndrome. It's a bit of a nightmare because it's associated with these things called superantigens. These are basically like the rogue agents of the bacterial world, and they're mostly coming from bugs like Staphylococcus aureus.\n",
      "\n",
      "Superantigens are tricky; they have a special way of activating T cells way beyond the norm. Usually, T cells need to see a specific antigen presented by MHC molecules, but these superantigens bypass that entire check and go straight to just supercharging T cells.\n",
      "\n",
      "So, how do they pull this off? Well, these superantigens physically link the MHC class II molecules, found on our antigen-presenting cells, directly to a very particular part of the T cell receptor. It's the variable region of the beta chain — known as TCR V beta. And bang! Just like that, the T cells are activated all at once without the usual specificity.\n",
      "\n",
      "When these T cells go into overdrive, they start pumping out a ton of cytokines. It’s a bit like setting off a massive immune fireworks show that leads to all these symptoms this woman is experiencing.\n",
      "\n",
      "So if we're honing in on one spot, the precise place these toxins make their move on the T cells is the TCR V beta region. Kind of diabolical, right? But it makes sense once you think about the whole bypass mechanism.\n",
      "\n",
      "And yeah, reviewing cases like this lined up with data shows that superantigens from bacteria like Staphylococcus aureus do indeed make a beeline for the TCR V beta region, pulling off this massive immune stimulation. So, the theory holds steady, and we're on the right track with this explanation.\n",
      "</think>\n",
      "\n",
      "The symptoms and scenario described align with toxic shock syndrome, commonly associated with superantigens produced by Staphylococcus aureus. These superantigens have a unique mechanism of action; they bypass normal antigen processing and directly stimulate T cells by binding to the T cell receptor (TCR). Specifically, the superantigens bind to the variable region of the beta chain of the T cell receptor's beta chain, known as TCR V beta. This interaction results in a massive, nonspecific activation of T cells, leading to the release of large amounts of cytokines and subsequent symptoms of toxic shock syndrome. Therefore, the specific molecule on T cells that the toxin most likely binds to is the TCR V beta region.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][100]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4086c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/idm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:412: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdattucodes\u001b[0m (\u001b[33mdeath-star\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/oracle/finetune_learn/wandb/run-20250820_170209-nkusqq1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/death-star/huggingface/runs/nkusqq1a' target=\"_blank\">grateful-elevator-57</a></strong> to <a href='https://wandb.ai/death-star/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/death-star/huggingface' target=\"_blank\">https://wandb.ai/death-star/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/death-star/huggingface/runs/nkusqq1a' target=\"_blank\">https://wandb.ai/death-star/huggingface/runs/nkusqq1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2463' max='2463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2463/2463 1:12:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.566200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.514900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2463, training_loss=1.5796280805531238, metrics={'train_runtime': 4378.1508, 'train_samples_per_second': 4.501, 'train_steps_per_second': 0.563, 'total_flos': 8.61730188023808e+16, 'train_loss': 1.5796280805531238, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./qwen3-medical-sft\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.03,  \n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",  \n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=50,\n",
    "    report_to=None,  \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=None,  \n",
    "    formatting_func=None,  \n",
    "    data_collator=None,  \n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd95673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let's think about diabetes treatment. So, we've got diabetes, which is a condition where the body doesn't handle sugar well. This can happen because the pancreas isn't making enough insulin or it's not working right. Hmm, insulin is crucial here because it helps move sugar from the blood into cells where it's needed for energy. Without it, sugar builds up in the blood, which is what we call hyperglycemia.\n",
      "\n",
      "Now, what do we do about it? Well, there are a few main approaches. First, there's lifestyle changes. That's probably the first thing you'd think of. Losing weight, eating better, and exercising can really help manage blood sugar levels. It's like giving the body a fresh start, so to speak.\n",
      "\n",
      "But then, there are medications. There are different types of drugs that can help control blood sugar. Insulin is one of them, but it's also possible to use oral medications like metformin. These meds help the body use sugar more efficiently. They're like nudges that help the body work better with insulin.\n",
      "\n",
      "Oh, and let's not forget about monitoring. You have to keep an eye on blood sugar levels because they can change a lot, and that's not good. Checking regularly helps make sure you're on track with your treatment.\n",
      "\n",
      "So, when it comes down to it, it's all about a combination of lifestyle changes and medications. Plus, regular monitoring is key to keeping everything in check. That's the whole picture of managing diabetes.\n",
      "</think>\n",
      "\n",
      "The treatment for diabetes typically involves a combination of lifestyle changes and medical interventions. \n",
      "\n",
      "Lifestyle changes include:\n",
      "1. **Weight Management**: Losing weight can significantly improve blood sugar control.\n",
      "2. **Healthy Diet**: Eating a balanced diet rich in fruits, vegetables, whole grains, and lean proteins while limiting sugary and high-fat foods.\n",
      "3. **Regular Exercise**: Physical activity helps improve insulin sensitivity and manage blood sugar levels.\n",
      "\n",
      "Medications are also essential:\n",
      "1. **Insulin**: This is often necessary for people with type 1 diabetes or those who have poor control with oral medications.\n",
      "2. **Oral Medications**: For type 2 diabetes, oral medications such as metformin, sulfonylureas, and DPP-4 inhibitors can help manage blood sugar levels.\n",
      "\n",
      "Regular monitoring of blood glucose levels is crucial to ensure that treatment is effective and make necessary adjustments. This ongoing management helps in controlling diabetes and preventing complications.\n",
      "tensor([151644,    872,    198,   3838,    374,    279,   6380,    369,  19754,\n",
      "            30, 151645,    198, 151644,  77091,    198, 151667,    198,  71486,\n",
      "            11,   1077,    594,   1744,    911,  19754,   6380,     13,   2055,\n",
      "            11,    582,   3003,   2684,  19754,     11,    892,    374,    264,\n",
      "          2971,   1380,    279,   2487,   3171,    944,   3705,  13163,   1632,\n",
      "            13,   1096,    646,   3537,   1576,    279,  61168,    300,   4436,\n",
      "           944,   3259,   3322,  31052,    476,    432,    594,    537,   3238,\n",
      "          1290,     13,  88190,     11,  31052,    374,  16587,   1588,   1576,\n",
      "           432,   8609,   3271,  13163,    504,    279,   6543,   1119,   7761,\n",
      "          1380,    432,    594,   4362,    369,   4802,     13,  17147,    432,\n",
      "            11,  13163,  22111,    705,    304,    279,   6543,     11,    892,\n",
      "           374,   1128,    582,   1618,  17071,  22945,     66,  21925,    382,\n",
      "          7039,     11,   1128,    653,    582,    653,    911,    432,     30,\n",
      "          8325,     11,   1052,    525,    264,   2421,   1887,  19827,     13,\n",
      "          5512,     11,   1052,    594,  18899,   4344,     13,   2938,    594,\n",
      "          4658,    279,   1156,   3166,    498,   4172,   1744,    315,     13,\n",
      "         98139,   4680,     11,  12182,   2664,     11,    323,  50482,    646,\n",
      "          2167,   1492,  10091,   6543,  13163,   5866,     13,   1084,    594,\n",
      "          1075,   7086,    279,   2487,    264,   7722,   1191,     11,    773,\n",
      "           311,   6468,    382,   3983,   1221,     11,   1052,    525,  29910,\n",
      "            13,   2619,    525,   2155,   4494,    315,  10975,    429,    646,\n",
      "          1492,   2524,   6543,  13163,     13,   9726,  23401,    374,    825,\n",
      "           315,   1105,     11,    714,    432,    594,   1083,   3204,    311,\n",
      "           990,  20655,  29910,   1075,   2270,    627,    258,     13,   4220,\n",
      "         83807,   1492,    279,   2487,    990,  13163,    803,  29720,     13,\n",
      "          2379,   2299,   1075,  41441,   4188,    429,   1492,    279,   2487,\n",
      "           975,   2664,    448,  31052,    382,  11908,     11,    323,   1077,\n",
      "           594,    537,  10667,    911,  16558,     13,   1446,    614,    311,\n",
      "          2506,    458,   7912,    389,   6543,  13163,   5866,   1576,    807,\n",
      "           646,   2297,    264,   2696,     11,    323,    429,    594,    537,\n",
      "          1661,     13,  46093,  15502,   8609,   1281,   2704,    498,   2299,\n",
      "           389,   3754,    448,    697,   6380,    382,   4416,     11,    979,\n",
      "           432,   4041,   1495,    311,    432,     11,    432,    594,    678,\n",
      "           911,    264,  10601,    315,  18899,   4344,    323,  29910,     13,\n",
      "         12343,     11,   5792,  16558,    374,   1376,    311,  10282,   4297,\n",
      "           304,   1779,     13,   2938,    594,    279,   4361,   6802,    315,\n",
      "         18150,  19754,    624, 151668,    271,    785,   6380,    369,  19754,\n",
      "         11136,  17601,    264,  10601,    315,  18899,   4344,    323,   6457,\n",
      "         38355,     13,   4710,     43,  16479,   4344,   2924,    510,     16,\n",
      "            13,   3070,   8295,   9551,  95518,  98139,   4680,    646,  11941,\n",
      "          7269,   6543,  13163,   2524,    624,     17,     13,   3070,  96113,\n",
      "         26212,  95518,  59538,    264,  23831,   9968,   9080,    304,  25322,\n",
      "            11,  23880,     11,   4361,  40836,     11,    323,  15651,  27796,\n",
      "          1393,  32894,  30605,    658,    323,   1550,  63254,  15298,    624,\n",
      "            18,     13,   3070,  30404,  32818,  95518,  27379,   5702,   8609,\n",
      "          7269,  31052,  26443,    323,  10091,   6543,  13163,   5866,    382,\n",
      "         13310,    292,    804,    525,   1083,   7565,    510,     16,     13,\n",
      "          3070,  15474,  23401,  95518,   1096,    374,   3545,   5871,    369,\n",
      "          1251,    448,    943,    220,     16,  19754,    476,   1846,    879,\n",
      "           614,   7852,   2524,    448,  20655,  29910,    624,     17,     13,\n",
      "          3070,   2195,    278,  12566,    804,  95518,   1752,    943,    220,\n",
      "            17,  19754,     11,  20655,  29910,   1741,    438,   2270,    627,\n",
      "           258,     11,  25774,  13172,     75,    552,    300,     11,    323,\n",
      "           422,   4406,     12,     19,  67542,    646,   1492,  10091,   6543,\n",
      "         13163,   5866,    382,  30404,  16558,    315,   6543,  33223,   5866,\n",
      "           374,  16587,    311,   5978,    429,   6380,    374,   7373,    323,\n",
      "          1281,   5871,  32974,     13,   1096,  14195,   6240,   8609,    304,\n",
      "         25902,  19754,    323,  26160,  35405,     13, 151645],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the treatment for diabetes?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text,return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "for token in model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "):\n",
    "    print(token)\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
